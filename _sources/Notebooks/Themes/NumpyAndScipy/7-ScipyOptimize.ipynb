{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163af418",
   "metadata": {},
   "source": [
    "# `scipy.optimize`\n",
    "\n",
    "\n",
    "See the following page for a detailed list of routines: [scipy.optimize reference docs](https://docs.scipy.org/doc/scipy/reference/optimize.html#module-scipy.optimize)\n",
    "\n",
    "The module contains:\n",
    "\n",
    "  - Unconstrained and constrained minimization of multivariate scalar functions (minimize) using a variety of algorithms (e.g. BFGS, Nelder-Mead simplex, Newton Conjugate Gradient, COBYLA or SLSQP)\n",
    "  - Global (brute-force) optimization routines (e.g. basinhopping, differential_evolution)\n",
    "  - Least-squares minimization (least_squares) and curve fitting (curve_fit) algorithms\n",
    "  - Scalar univariate functions minimizers (minimize_scalar) and root finders (newton)\n",
    "  - Multivariate equation system solvers (root) using a variety of algorithms (e.g. hybrid Powell, Levenberg-Marquardt or large-scale methods such as Newton-Krylov).\n",
    "  \n",
    "Here are a couple of useful examples for simple problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f29dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4324871",
   "metadata": {},
   "source": [
    "## Curve fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49384c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b3474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, a, b, c):\n",
    "    \"\"\"\n",
    "    Fitting function for the data\n",
    "    \"\"\"\n",
    "    return a * np.exp(-b * x) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = np.linspace(0, 4, 500)\n",
    "y = func(xdata, 2.5, 1.3, 0.5)\n",
    "y_noise = 0.2 * np.random.normal(size=xdata.size)\n",
    "ydata = y + y_noise\n",
    "plt.plot(xdata, ydata, 'b-', label='data')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e06b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit for the parameters a, b, c of the function func\n",
    "popt, pcov = curve_fit(func, xdata, ydata)\n",
    "print (\"func(x, a={}, b={}, c={})\".format(popt[0], popt[1], popt[2]))\n",
    "\n",
    "# Now func (x, *popt) will give the y values of the fitted function\n",
    "\n",
    "plt.plot(xdata, ydata, 'b-', label='data')\n",
    "plt.plot(xdata, func(xdata, *popt), 'r-', label='fit', linewidth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46faba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constraints on a, b, c\n",
    "poptc, pcovc = curve_fit(func, xdata, ydata, bounds=([0.,0.,1.], [3.0, 3.0, 1.5]))\n",
    "print (\"func(x, a={}, b={}, c={})\".format(poptc[0], poptc[1], poptc[2]))\n",
    "\n",
    "plt.plot(xdata, ydata, 'b-', label='data')\n",
    "plt.plot(xdata, func(xdata, *popt), 'r-', label='fit', linewidth=3)\n",
    "plt.plot(xdata, func(xdata, *poptc), 'g-', label='fit-with-bounds', linewidth=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59137c5",
   "metadata": {},
   "source": [
    "## Least squares\n",
    "\n",
    "We can also use the general least squares machinery for a more adjustable curve fitting method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9443d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import least_squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3bd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_data(t, a, b, c, noise=0, n_outliers=0, random_state=0):\n",
    "     y = a + b * np.exp(t * c)\n",
    "\n",
    "     rnd = np.random.RandomState(random_state)\n",
    "     error = noise * rnd.randn(t.size)\n",
    "     outliers = rnd.randint(0, t.size, n_outliers)\n",
    "     error[outliers] *= 10\n",
    "\n",
    "     return y + error\n",
    "\n",
    "a = 0.5\n",
    "b = 2.0\n",
    "c = -1\n",
    "t_min = 0\n",
    "t_max = 10\n",
    "n_points = 15\n",
    "\n",
    "t_train = np.linspace(t_min, t_max, n_points)\n",
    "y_train = gen_data(t_train, a, b, c, noise=0.1, n_outliers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38038c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting function of three parameters in 2 variables\n",
    "def fun(x, t, y):\n",
    "    return x[0] + x[1] * np.exp(x[2] * t) - y\n",
    "\n",
    "x0 = np.array([1.0, 1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5371d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the least squares problem\n",
    "\n",
    "res_lsq = least_squares(fun, x0, args=(t_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8325aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the lsq with two different robust loss functions. \n",
    "# The parameter f_scale is set to 0.1, meaning that inlier \n",
    "# residuals should not significantly exceed 0.1 (the noise level used).\n",
    "\n",
    "res_soft_l1 = least_squares(fun, x0, loss='soft_l1', f_scale=0.1,\n",
    "                            args=(t_train, y_train))\n",
    "\n",
    "res_log = least_squares(fun, x0, loss='cauchy', f_scale=0.1,\n",
    "                        args=(t_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dense arrays of these fits to plot\n",
    "\n",
    "t_test = np.linspace(t_min, t_max, n_points * 10)\n",
    "y_true = gen_data(t_test, a, b, c)\n",
    "y_lsq = gen_data(t_test, *res_lsq.x)\n",
    "y_soft_l1 = gen_data(t_test, *res_soft_l1.x)\n",
    "y_log = gen_data(t_test, *res_log.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c0fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_train, y_train, 'o')\n",
    "plt.plot(t_test, y_true, 'k', linewidth=2, label='true')\n",
    "plt.plot(t_test, y_lsq, label='linear loss')\n",
    "plt.plot(t_test, y_soft_l1, label='soft_l1 loss')\n",
    "plt.plot(t_test, y_log, label='cauchy loss')\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1920e3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08104458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.12,
    "jupytext_version": "1.6.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   29,
   34,
   38,
   42,
   50,
   59,
   70,
   78,
   84,
   88,
   110,
   118,
   124,
   136,
   146,
   158,
   162
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}